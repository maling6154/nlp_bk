{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter\n",
    "import six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 10K raw file and output textual features for classification using sklearn and keras.  \n",
    "Inputs: \n",
    "- data/10k/10k_raw.pickle: a pandas pickle file containing gvkey, fyear, and MD&A section \n",
    "\n",
    "Outputs:\n",
    "- data/10k/10k_index.csv: an index file that preserves the sequence \n",
    "- data/10k/X_keras_unigram.npy: unigram sequences for keras\n",
    "- data/10k/word_map.pickle: a dictionary to map numbers to words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load 10k raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017-04-18 Use full 10K sample (included 10KSB etc.)  \n",
    "Inputs\n",
    "- mda_raw_corpus.pickle: Generated from `process_10k` file in the culture project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_10k = pd.read_pickle(\"/shared/data/10k_2017/processed_corpus/mda_raw_corpus.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234701, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_10k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sec10k_index = sec_10k[['gvkey','fyear']]\n",
    "sec10k_index.to_csv(\"/shared/data/10k_2017/processed_corpus/10k_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove obs with no MD&A\n",
    "sec_10k_no_NA = sec_10k[sec_10k['mda_text'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sec_10k_no_NA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_10k_no_NA.to_pickle(\"/shared/data/10k_2017/processed_corpus/mda_raw_corpus_no_none.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec10k_index_no_NA = sec_10k_no_NA[['gvkey','fyear']]\n",
    "sec10k_index_no_NA.to_csv(\"/shared/data/10k_2017/processed_corpus/10k_index_no_none.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use keras to pre-process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = 10000\n",
    "mda_text_list = sec_10k['mda_text'].tolist()\n",
    "del sec_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(mda_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(file=open(\"data/10k/tokenizer.pickle\", 'rb'))\n",
    "tokenizer.nb_words = nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_tokenizer_word_index(word_index, word_counts, rare_threshold = 5, most_common = 100):\n",
    "    '''filter out the words to include in corpus'''\n",
    "    # filter out any token that has no letter \n",
    "    word_index ={word: v for word, v in word_index.items() if (any(c.isalpha() for c in word) and len(word)>=3 and len(word) <=18)}\n",
    "    word_counts ={word: v for word, v in word_counts.items() if (any(c.isalpha() for c in word) and len(word)>=3 and len(word) <=18)}\n",
    "    \n",
    "    # filter out html codes and other words\n",
    "    to_filter = set(['div', 'align', 'border', 'color', 'font', 'left', 'colspan', 'roman', 'roman\\'','valign', 'family', 'hidden', 'bottom', 'times', 'padding', 'rowspan', 'background', 'class', 'cceeff',\n",
    "                    'style', 'text', 'medium', 'vertical', 'nbsp', 'width', 'nowrap', 'serif', 'indent', 'height', 'inherit', 'offset','msonormal', 'weight',\n",
    "                    'top', 'right', 'none'])\n",
    "    p = re.compile(\"\\d+(pt|px|in)\")\n",
    "    word_index ={word: v for word, v in word_index.items() if (not bool(p.match(word))) and word not in to_filter}\n",
    "    word_counts ={word: v for word, v in word_counts.items() if (not bool(p.match(word))) and word not in to_filter}\n",
    "    \n",
    "    \n",
    "    # filter most common and rare words\n",
    "    most_common_words = set(sorted(word_counts, key=word_counts.get, reverse=True)[:most_common])\n",
    "    print(most_common_words)\n",
    "    word_index ={word: v for word, v in word_index.items() if (word_counts.get(word) >= rare_threshold) and (word not in most_common_words)}\n",
    "    word_counts ={word: v for word, v in word_counts.items() if (word_counts.get(word) >= rare_threshold) and (word not in most_common_words)}\n",
    "    \n",
    "    return word_index, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the raw text\n",
    "tokenizer = Tokenizer(nb_words=nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "filtered_tokenizer_dicts = filter_tokenizer_word_index(tokenizer.word_index, tokenizer.word_counts, most_common= 0)\n",
    "tokenizer.word_index = filtered_tokenizer_dicts[0]\n",
    "tokenizer.word_counts =  filtered_tokenizer_dicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241953"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(mda_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X is a matrix, each row is sequence of word ids\n",
    "X_seq = tokenizer.texts_to_sequences(mda_text_list) \n",
    "X = np.array(X_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the tokenized sequence to disk\n",
    "np.save(\"data/10k/X_keras_unigram.npy\", X)\n",
    "# np.save(\"data/10k/X_keras_unigram_20000.npy\", X)\n",
    "# save the word mapping to disk\n",
    "pickle.dump(tokenizer.word_index, file=open(\"data/10k/word_map.pickle\", 'wb'))\n",
    "# pickle.dump(tokenizer.word_index, file=open(\"data/10k/word_map_20000.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the trained tokenizer\n",
    "# pickle.dump(tokenizer, file=open(\"data/10k/tokenizer.pickle\", 'wb'))\n",
    "# pickle.dump(tokenizer, file=open(\"data/10k/tokenizer_20000.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf = tokenizer.sequences_to_matrix(X_seq, mode = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tfidf = np.array(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"data/10k/X_tfidf.npy\", X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reimburses',\n",
       " 'costless',\n",
       " 'reside',\n",
       " 'elk',\n",
       " 'ultrasound',\n",
       " 'contemplation',\n",
       " 'corps',\n",
       " 'systemic',\n",
       " 'qspe',\n",
       " 'knee']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.word_index, key=tokenizer.word_index.get)[10000:10010]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
