{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, Convolution2D, Convolution1D, Reshape, Lambda, AveragePooling1D, AveragePooling2D, MaxPooling1D\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.regularizers import l1, l2, activity_l2, l1, activity_l1\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 5000 # max number of words to include (remove lower frequency words)\n",
    "maxlen = 5000  # cut texts after this number of words \n",
    "\n",
    "test_train_split_year = 2011\n",
    "forecast_year = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_year_before(df, n = 1):\n",
    "    \"\"\"input x,y df, return df with y and n year before x\"\"\"\n",
    "    dat_tmp = df.copy()\n",
    "    dat_tmp['fyear'] = dat_tmp['fyear'] + n\n",
    "    dat_tmp = dat_tmp.drop('Y',axis =1)\n",
    "    Ys = df[['fyear','gvkey','Y']]\n",
    "    n_year = pd.merge(dat_tmp,Ys,how = 'inner',on=['fyear','gvkey'])\n",
    "    return n_year\n",
    "\n",
    "\n",
    "def load_data(X_padded_text):\n",
    "    \"\"\" Load tokenized word sequence and pad it for deep learning\"\"\"   \n",
    "    index_10k = pd.read_csv('data/10k/10k_index.csv',usecols=['gvkey','fyear'])\n",
    "    index_10k['index_10k'] = index_10k.index\n",
    "    final_variable = pd.read_csv('data/final_variables.csv')\n",
    "    final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "    final_variable = final_variable.replace([np.inf,-np.inf],0)\n",
    "\n",
    "    # match text index with one year after y, index_10k_y has the index of text data has one year after Y matched\n",
    "    index_10k_y = pd.merge(left=index_10k, right=final_variable, how='inner', on=['gvkey','fyear'])\n",
    "    print(\"Total number of observations with no forecasting: \")\n",
    "    print(index_10k_y.shape)\n",
    "    \n",
    "    index_10k_y_n_year = n_year_before(index_10k_y, n = forecast_year)\n",
    "    print(\"Total number of observations: \")\n",
    "    print(index_10k_y_n_year.shape)\n",
    "    \n",
    "    y = np.array(index_10k_y_n_year['Y'])\n",
    "    X_text = X_padded_text[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "    X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "    X_num = X_num.as_matrix()\n",
    "    # split train-test by year\n",
    "    index_10k_y_n_year = index_10k_y_n_year.reset_index(drop=True)\n",
    "    train_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] < test_train_split_year].index.tolist()\n",
    "    test_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] >= test_train_split_year].index.tolist()\n",
    "    print(X_text.shape, X_num.shape, y.shape)\n",
    "    return X_text, X_num, train_index, test_index, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with no forecasting: \n",
      "(79222, 44)\n",
      "Total number of observations: \n",
      "(64999, 44)\n",
      "(64999, 5000) (64999, 36) (64999,)\n"
     ]
    }
   ],
   "source": [
    "X_padded = pickle.load(open(\"data/10k/X_padded.pickle\", 'rb'))\n",
    "X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 50\n",
    "batch_size = 32\n",
    "nb_epoch = 8\n",
    "class_weight = {0:1,1:50}\n",
    "\n",
    "def create_model_text(no_merge = False):\n",
    "    # this is the Deep Averaging Network Moodel\n",
    "    # see \"Deep Unordered Composition Rivals Syntactic Methods for Text Classification\", Iyyer et al. 2015\n",
    "\n",
    "    embedding_size = embedding_dims\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "\n",
    "#     model.add(Dropout(0.1)) # Use dropout if implement the DAN model\n",
    "    model.add(AveragePooling1D(pool_length=model.output_shape[1]))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(8, activation='relu', activity_regularizer=activity_l2(0.1)))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_num(no_merge = False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=36, activation='relu', activity_regularizer=activity_l2(0.00001)))\n",
    "#     model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_merge_layer():\n",
    "    model_text = create_model_text()\n",
    "    model_num = create_model_num()\n",
    "    merged = Merge([model_text, model_num], mode='concat', concat_axis = 1)\n",
    "    final_model = Sequential()\n",
    "    final_model.add(merged)\n",
    "    print(final_model.layers[-1].output_shape)\n",
    "    final_model.add(Dense(4, activation='relu', activity_regularizer=activity_l2(0.00001)))\n",
    "    final_model.add(Dense(1, activation='sigmoid'))\n",
    "    final_model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "    return final_model\n",
    "\n",
    "\n",
    "def get_date_time():\n",
    "    d_date = datetime.datetime.now()\n",
    "    reg_format_date = d_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return(reg_format_date)\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            score = metrics.roc_auc_score(self.y_val, y_pred)\n",
    "            logs['score'] = score\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))\n",
    "            \n",
    "            \n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, \n",
    "                             model_output, model_config_output,\n",
    "                             batch_size = batch_size, nb_epoch = nb_epoch, verbose = 1,\n",
    "                            ):\n",
    "    \"\"\"Calculate and print ROC score from a set of X and y\"\"\"\n",
    "    ival = IntervalEvaluation(validation_data=(X_test, y_test), interval=1)\n",
    "    early_stopping = EarlyStopping(monitor='score', min_delta=0, patience=2, mode = 'max')\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, \n",
    "             validation_data= None, class_weight = class_weight, verbose = verbose, callbacks=[ival, early_stopping])\n",
    "    pred_yp = model.predict(X_test)\n",
    "    roc = metrics.roc_auc_score(y_test , pred_yp)\n",
    "    accuracy_ratio = (roc-0.5)*2 \n",
    "    brier = metrics.brier_score_loss(y_test , pred_yp)\n",
    "    date_time_stamp = get_date_time()\n",
    "    model.save(model_output + date_time_stamp)\n",
    "    with open(model_config_output + date_time_stamp, 'w') as file:\n",
    "        file.write(model.to_json())\n",
    "    print(\"AUC \" + str(roc))\n",
    "    print(\"Accuracy_ratio \" + str(accuracy_ratio))\n",
    "    print(\"Brier Score \" + str(brier))\n",
    "    with open(\"model/dl_log.txt\", 'a') as file:\n",
    "        file.write(date_time_stamp + \"\\n\" + str([accuracy_ratio, roc, brier]) + \"\\n\")\n",
    "    return [accuracy_ratio, roc, brier]\n",
    "\n",
    "\n",
    "\n",
    "def forecast_performace(X_text, X_num, train_index, test_index, y):\n",
    "    \"\"\"Train_test split using year; Calculate and print performance score\"\"\"\n",
    "    print(\"Running Model\")\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model_merge_layer()\n",
    "    X_train = [X_text[train_index], X_num[train_index]]\n",
    "    y_train = y[train_index]\n",
    "    X_test = [X_text[test_index], X_num[test_index]]\n",
    "    y_test =  y[test_index]\n",
    "    train_and_evaluate_model(model, X_train, y_train, X_test, y_test, model_output = 'model/DL_merge_.mod',\n",
    "                                model_config_output = \"model/DL_merge.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_performace(X_text, X_num, train_index, test_index, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
