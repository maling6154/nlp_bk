{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bankruptcy Prediction using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016-12-20, Feng Mai  \n",
    "Use Keras merge layer to combine textual and numerical features for bankruptcy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Activation, Embedding, Flatten, Convolution2D, Convolution1D, Reshape, Lambda, AveragePooling1D, AveragePooling2D, MaxPooling1D\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, GlobalMaxPooling2D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000 # max number of words to include (remove lower frequency words)\n",
    "maxlen = 5000  # cut texts after this number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_year_before(df, n = 1):\n",
    "    \"\"\"input x,y df, return df with y and n year before x\"\"\"\n",
    "    dat_tmp = df.copy()\n",
    "    dat_tmp['fyear'] = dat_tmp['fyear'] + n\n",
    "    dat_tmp = dat_tmp.drop('Y',axis =1)\n",
    "    Ys = df[['fyear','gvkey','Y']]\n",
    "    n_year = pd.merge(dat_tmp,Ys,how = 'inner',on=['fyear','gvkey'])\n",
    "    return n_year\n",
    "\n",
    "\n",
    "def pad_text_data():\n",
    "    \"\"\" Load tokenized word sequence and pad it for deep learning\"\"\"\n",
    "    print('Loading data')\n",
    "    X = np.load(\"data/10k/X_keras_unigram.npy\")\n",
    "    lengths = [len(x) for x in X]\n",
    "    pd.Series(lengths).describe()\n",
    "    # pad sequence for deep learning\n",
    "    print('Pad sequences')\n",
    "    X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def construct_aligned_num_matrix(forecast_year = 1):\n",
    "    index_10k = pd.read_csv('/shared/data/10k_2017/processed_corpus/10k_index.csv',usecols=['gvkey','fyear'])\n",
    "    index_10k['index_10k'] = index_10k.index\n",
    "    final_variable = pd.read_csv('data/final_variables.csv')\n",
    "    final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "    final_variable = final_variable.replace([np.inf,-np.inf],0)\n",
    "\n",
    "    final_variable = final_variable.query('fyear <= 2014 & fyear >= 1993')\n",
    "    # match text index with one year after y, index_10k_y has the index of text data has one year after Y matched\n",
    "    index_10k_y = pd.merge(left=index_10k, right=final_variable, how='inner', on=['gvkey','fyear'])\n",
    "    print(\"Total number of observations with no forecasting: \")\n",
    "    print(index_10k_y.shape)\n",
    "\n",
    "    index_10k_y_n_year = n_year_before(index_10k_y, n = forecast_year)\n",
    "    print(\"Total number of observations: \")\n",
    "    print(index_10k_y_n_year.shape)\n",
    "    # split train-test by year\n",
    "    index_10k_y_n_year = index_10k_y_n_year.sort_values(['gvkey', 'fyear'], ascending=[1, 1])\n",
    "    index_10k_y_n_year = index_10k_y_n_year.reset_index(drop=True)\n",
    "    return index_10k_y_n_year\n",
    "    \n",
    "    \n",
    "def load_data(X_padded_text, forecast_year = 1, random_split = False, random_state = 1001,\n",
    "              test_train_split_year = None):\n",
    "    \"\"\" Load tokenized word sequence and pad it for deep learning\n",
    "    If random_split, then treat it as a classification problem\n",
    "    If random_split is false, use test_train_split_year to split the dataset into training and spliting\n",
    "    \"\"\"   \n",
    "    index_10k_y_n_year = construct_aligned_num_matrix(forecast_year=1)\n",
    "\n",
    "    if random_split:\n",
    "        all_index = index_10k_y_n_year.index.tolist()\n",
    "        split = sklearn.model_selection.train_test_split(all_index, train_size = 0.8, random_state=random_state)\n",
    "        pickle.dump(split, file=open(\"data/split.pickle_\" + str(forecast_year) + \"r\" + str(random_state), \"wb\"))\n",
    "        train_index = split[0]\n",
    "        test_index = split[1]\n",
    "    else:\n",
    "        train_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] < test_train_split_year].index.tolist()\n",
    "        test_index = index_10k_y_n_year[index_10k_y_n_year['fyear'] >= test_train_split_year].index.tolist()\n",
    "    \n",
    "    y = np.array(index_10k_y_n_year['Y'])\n",
    "    X_text = X_padded_text[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "    X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "    X_num = X_num.as_matrix()\n",
    "    print(len(train_index), len(test_index))\n",
    "    return X_text, X_num, train_index, test_index, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_ten_by_ten_split(X_padded_text, forecast_year = 1, random_state = 1001):\n",
    "    '''\n",
    "    Dump the indicies (list of kfold splits) for 10x10 cv\n",
    "    '''\n",
    "    index_10k_y_n_year = construct_aligned_num_matrix(forecast_year=forecast_year)\n",
    "\n",
    "    ten_by_ten_splits = []\n",
    "    all_index = index_10k_y_n_year.index.tolist()\n",
    "    for i in range(10):\n",
    "        all_index = shuffle(all_index, random_state=1001)\n",
    "        kf = KFold(n_splits = 10)\n",
    "        splits = kf.split(all_index)\n",
    "        ten_by_ten_splits.append(list(splits))\n",
    "        pickle.dump(ten_by_ten_splits, file=open(\"data/ten_by_ten_splits.pickle_\" + str(forecast_year) + \"r\" + str(random_state), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_text():\n",
    "    X_padded = pad_text_data()\n",
    "    pickle.dump(X_padded, open(\"data/10k/X_padded.pickle\", 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_padded = pickle.load(open(\"data/10k/X_padded.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_ten_by_ten_split(X_padded, forecast_year = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with no forecasting: \n",
      "(95987, 44)\n",
      "Total number of observations: \n",
      "(73112, 44)\n"
     ]
    }
   ],
   "source": [
    "dump_ten_by_ten_split(X_padded, forecast_year = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with no forecasting: \n",
      "(95987, 44)\n",
      "Total number of observations: \n",
      "(63762, 44)\n"
     ]
    }
   ],
   "source": [
    "dump_ten_by_ten_split(X_padded, forecast_year = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_year = 1 \n",
    "index_10k = pd.read_csv('/shared/data/10k_2017/processed_corpus/10k_index.csv',usecols=['gvkey','fyear'])\n",
    "index_10k['index_10k'] = index_10k.index\n",
    "final_variable = pd.read_csv('data/final_variables.csv')\n",
    "final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "final_variable = final_variable.replace([np.inf,-np.inf],0)\n",
    "final_variable = final_variable.query('fyear <= 2014 & fyear >=1994')\n",
    "\n",
    "# match text index with one year after y, index_10k_y has the index of text data has one year after Y matched\n",
    "index_10k_y = pd.merge(left=index_10k, right=final_variable, how='inner', on=['gvkey','fyear'])\n",
    "print(\"Total number of observations with no forecasting: \")\n",
    "print(index_10k_y.shape)\n",
    "\n",
    "index_10k_y_n_year = n_year_before(index_10k_y, n = forecast_year)\n",
    "\n",
    "print(\"Total number of observations with forecasting: \")\n",
    "print(index_10k_y_n_year.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(final_variable['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(index_10k_y['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_10k_y.to_csv(\"data/final_sample.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_10k_y_n_year.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_10k_y[['fyear', 'Y']].groupby('fyear').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define different deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "embedding_dims = 20\n",
    "\n",
    "# def create_model_text(no_merge = False):\n",
    "#     # this is the Deep Averaging Network Moodel\n",
    "#     # see \"Deep Unordered Composition Rivals Syntactic Methods for Text Classification\", Iyyer et al. 2015\n",
    "\n",
    "#     embedding_size = embedding_dims\n",
    "#     print('Build model...')\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "# #     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "\n",
    "#     model.add(Dense(4, activity_regularizer=l2(0.01)))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dense(4, activity_regularizer=l2(0.01)))\n",
    "#     model.add(Activation('relu'))\n",
    "    \n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1, activation='sigmoid'))\n",
    "# #         print(model.layers[-1].output_shape)\n",
    "\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='nadam',\n",
    "#                   metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "def create_model_text(no_merge = False, archi = \"embedding\"):\n",
    "    if archi == \"embedding\":\n",
    "        # this is the fast-text model\n",
    "        model = Sequential()\n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        model.add(Embedding(max_features,\n",
    "                            embedding_dims,\n",
    "                            input_length=maxlen))\n",
    "        # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "        # of all words in the document\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        if no_merge:\n",
    "            # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    elif archi == \"CNN\":\n",
    "        # this is a CNN from Keras example\n",
    "        nb_filter = 100\n",
    "        filter_length = 4\n",
    "        hidden_dims = 100\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Embedding(max_features,\n",
    "                            embedding_dims,\n",
    "                            input_length=maxlen))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # we add a Convolution1D, which will learn filters\n",
    "        # word group filters of size filter_length:\n",
    "        model.add(Conv1D(nb_filter,\n",
    "                         filter_length,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        # we use max pooling:\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "\n",
    "        # We add a vanilla hidden layer:\n",
    "        model.add(Dense(hidden_dims))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        if no_merge:\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# def create_model_text(no_merge = False):\n",
    "#     # this is a 2D CNN\n",
    "#     nb_filter = 50\n",
    "#     filter_length = 3\n",
    "#     hidden_dims = 4\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Embedding(max_features,\n",
    "#                         embedding_dims,\n",
    "#                         input_length=maxlen))\n",
    "#     print(model.layers[-1].output_shape)\n",
    "#     model.add(Reshape((maxlen, embedding_dims, 1)))\n",
    "#     print(model.layers[-1].output_shape)\n",
    "#     model.add(Convolution2D(nb_filter = nb_filter, nb_col= embedding_dims,  nb_row= filter_length, \n",
    "#                             border_mode='same'))\n",
    "#     model.add(Activation(\"relu\"))\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # we use max pooling:\n",
    "#     model.add(GlobalMaxPooling2D())\n",
    "#     print(model.layers[-1].output_shape)\n",
    "\n",
    "#     # We add a vanilla hidden layer:\n",
    "#     model.add(Dense(hidden_dims))\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1))\n",
    "#         model.add(Activation('sigmoid'))\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer='adam',\n",
    "#                       metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "        \n",
    "def create_model_num(no_merge = False):\n",
    "    # simple model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=36, activation='relu', activity_regularizer=l1(0.0001)))   \n",
    "    if no_merge:\n",
    "        model.add(Dense(1, activation='sigmoid',init='zero'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# def create_model_num(no_merge = False):\n",
    "#     # deep model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(4, input_dim=36, activation='relu'))  \n",
    "#     model.add(Dense(4, activation='relu'))   \n",
    "#     model.add(Dense(4, activation='relu'))   \n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1, activation='sigmoid',init='zero'))\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                           optimizer='adam',\n",
    "#                           metrics=['accuracy'])\n",
    "#     return model\n",
    "              \n",
    "              \n",
    "# def create_model_num(no_merge = False):\n",
    "#     # wider model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(16, input_dim=36, activation='relu', activity_regularizer=l2(0.0001)))\n",
    "#     if no_merge:\n",
    "#         model.add(Dense(1, activation='sigmoid',init='zero'))\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                           optimizer='adam',\n",
    "#                           metrics=['accuracy'])\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 6\n",
    "class_weight = {0:1,1:1}\n",
    "\n",
    "def get_date_time():\n",
    "    d_date = datetime.datetime.now()\n",
    "    reg_format_date = d_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return(reg_format_date)\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    \"\"\" Show AUC after interval number of epoches \"\"\"\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            score = metrics.roc_auc_score(self.y_val, y_pred)\n",
    "            logs['score'] = score\n",
    "            print(\" epoch:{:d} AUC: {:.4f}\".format(epoch, score))\n",
    "            \n",
    "            \n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, \n",
    "                             model_output, model_config_output,\n",
    "                             batch_size = batch_size, nb_epoch = nb_epoch, verbose = 1,\n",
    "                            ):\n",
    "    \"\"\"Calculate and print ROC score from a set of X and y\"\"\"\n",
    "    ival = IntervalEvaluation(validation_data=(X_test, y_test), interval=1)\n",
    "    early_stopping = EarlyStopping(monitor='score', min_delta=0.0, patience=500, mode = 'max')\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch, \n",
    "             validation_data= None, class_weight = class_weight, verbose = verbose, callbacks=[ival, early_stopping])\n",
    "    pred_yp = model.predict(X_test)\n",
    "    roc = metrics.roc_auc_score(y_test , pred_yp)\n",
    "    accuracy_ratio = (roc-0.5)*2 \n",
    "    brier = metrics.brier_score_loss(y_test , pred_yp)\n",
    "    date_time_stamp = get_date_time()\n",
    "    model.save(model_output + date_time_stamp)\n",
    "    with open(model_config_output + date_time_stamp, 'w') as file:\n",
    "        file.write(model.to_json())\n",
    "    print(\"AUC \" + str(roc))\n",
    "    print(\"Accuracy_ratio \" + str(accuracy_ratio))\n",
    "    print(\"Brier Score \" + str(brier))\n",
    "    with open(\"model/dl_log.txt\", 'a') as file:\n",
    "        file.write(date_time_stamp + \"\\n\" + str([accuracy_ratio, roc, brier]) + \"\\n\")\n",
    "    return [accuracy_ratio, roc, brier]\n",
    "\n",
    "\n",
    "def forecast_performace(X_text, X_num, train_index, test_index, y, model_output_path, \n",
    "                        model_type = \"text\", archi = \"embedding\"):\n",
    "    \"\"\"Train_test split using year; Calculate and print performance score\"\"\"\n",
    "    print(\"Running Model\")\n",
    "    model = None # Clearing the NN.\n",
    "    \n",
    "    if model_type == \"text\":\n",
    "        model = create_model_text(no_merge=True, archi = archi)\n",
    "        X_train = X_text[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X_text[test_index]\n",
    "        y_test = y[test_index]\n",
    "        return train_and_evaluate_model(model, X_train, y_train, X_test, y_test, \n",
    "                                        model_output = model_output_path + \".mod\",\n",
    "                                        model_config_output = model_output_path + \".json\", verbose = 0)\n",
    "        \n",
    "    if model_type == \"num\":\n",
    "        model = create_model_num(no_merge=True)\n",
    "        X_train = X_num[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X_num[test_index]\n",
    "        y_test = y[test_index]\n",
    "        return train_and_evaluate_model(model, X_train, y_train, X_test, y_test,\n",
    "                                 model_output = model_output_path + \".mod\",\n",
    "                                 model_config_output = model_output_path + \".json\",\n",
    "                                 verbose = 0)\n",
    "\n",
    "    if model_type == \"merge\":\n",
    "        model = create_model_merge_layer()\n",
    "        X_train = [X_text[train_index], X_num[train_index]]\n",
    "        y_train = y[train_index]\n",
    "        X_test = [X_text[test_index], X_num[test_index]]\n",
    "        y_test =  y[test_index]\n",
    "        return train_and_evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    \n",
    "\n",
    "def kfold_performace(X_text, X_num, y, n_folds = 2):\n",
    "    \"\"\"Calculate and print average stratified K-fold performance score\"\"\"\n",
    "    skf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    performance_scores = []\n",
    "    for i, (train_index, test_index) in enumerate(skf):\n",
    "        print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "        model = None # Clearing the NN.\n",
    "        model = create_model_merge_layer()\n",
    "        performance_scores.append(train_and_evaluate_model(model, X_text, X_num, y, train_index, test_index, \n",
    "                                                          just_text=False))\n",
    "    print(sum(performance_scores)/n_folds)\n",
    "\n",
    "#kfold_performace(X, y, n_folds = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for random_state in range(1000, 1010):\n",
    "    X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded, \n",
    "                                                          forecast_year = 1, \n",
    "                                                          random_split=True, random_state = random_state)\n",
    "    print(sum(y[train_index]), sum(y[test_index]))\n",
    "    AUC = forecast_performace(X_text, X_num, train_index, test_index, y, \"text\")[1]\n",
    "    with open(\"model/text_by_random.txt\", 'a') as file:\n",
    "        file.write(\"CNN-50\"+ \", \"+ str(random_state) + \", \" + str(1) + \", \" + str(AUC) + \"\\n\")\n",
    "    AUC = forecast_performace(X_text, X_num, train_index, test_index, y, \"num\")[1]\n",
    "#     with open(\"model/num_by_random.txt\", 'a') as file:\n",
    "#         file.write(\"DL\"+ \", \"+ str(random_state) + \", \" + str(1) + \", \" + str(AUC) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded, \n",
    "                                                      forecast_year = 1, \n",
    "                                                      random_split=True, random_state = 1004)\n",
    "print(sum(y[train_index]), sum(y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_performace(X_text, X_num, train_index, test_index, y, \"model/text_cnn_1004_1y\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_performace(X_text, X_num, train_index, test_index, y, \"model/num_wider_1004_1y\", \"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in range(2005, 2013):\n",
    "    print(year)\n",
    "    X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded, forecast_year = 1, random_split=False, test_train_split_year=year)\n",
    "    print(sum(y[train_index]), sum(y[test_index]))\n",
    "    AUC = forecast_performace(X_text, X_num, train_index, test_index, y, \"text\")[1]\n",
    "    with open(\"model/text_by_year.txt\", 'a') as file:\n",
    "        file.write(\"Embedding_avg\"+ \", \"+ str(year) + \", \" + str(1) + \", \" + str(AUC) + \"\\n\")\n",
    "    AUC = forecast_performace(X_text, X_num, train_index, test_index, y, \"num\")[1]\n",
    "    with open(\"model/num_by_year.txt\", 'a') as file:\n",
    "        file.write(\"DL\"+ \", \"+ str(year) + \", \" + str(1) + \", \" + str(AUC) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 x 10 cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer forecasting horizon y =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations with no forecasting: \n",
      "(95987, 44)\n",
      "Total number of observations: \n",
      "(73112, 44)\n"
     ]
    }
   ],
   "source": [
    "forecast_year = 2\n",
    "ten_by_ten_split = pickle.load(open(\"data/ten_by_ten_splits.pickle_2r1001\", 'rb'))\n",
    "index_10k_y_n_year = construct_aligned_num_matrix(forecast_year=forecast_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65800 7312\n",
      "272 26\n",
      "Running Model\n",
      "<keras.models.Sequential object at 0x7f59ff2673c8>\n",
      " epoch:0 AUC: 0.6290\n",
      " epoch:1 AUC: 0.6919\n",
      " epoch:2 AUC: 0.7100\n",
      " epoch:3 AUC: 0.7301\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 6\n",
    "\n",
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), model_type = \"text\", archi = \"embedding\")\n",
    "        with open(\"model/dl_text_embedding_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), model_type = \"text\", archi = \"CNN\")\n",
    "        with open(\"model/dl_text_cnn_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), \"num\")\n",
    "        with open(\"model/dl_num_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longer forecasting horizon y =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_year = 3\n",
    "ten_by_ten_split = pickle.load(open(\"data/ten_by_ten_splits.pickle_3r1001\", 'rb'))\n",
    "index_10k_y_n_year = construct_aligned_num_matrix(forecast_year=forecast_year)\n",
    "nb_epoch = 6\n",
    "\n",
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), model_type = \"text\", archi = \"embedding\")\n",
    "        with open(\"model/dl_text_embedding_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")\n",
    "            \n",
    "            \n",
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), model_type = \"text\", archi = \"CNN\")\n",
    "        with open(\"model/dl_text_cnn_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "    for inner_loop, fold in enumerate(outer_split):\n",
    "        train_index = fold[0]\n",
    "        test_index = fold[1]\n",
    "        y = np.array(index_10k_y_n_year['Y'])\n",
    "        X_text = X_padded[index_10k_y_n_year['index_10k'].tolist()] # get all text X which has matched one year after Y\n",
    "        X_num = index_10k_y_n_year.drop(['gvkey', 'fyear', 'datadate', 'cusip', 'PERMCO', 'Y', 'PERMNO', 'index_10k'], 1)\n",
    "        X_num = X_num.as_matrix()\n",
    "        print(len(train_index), len(test_index))\n",
    "        print(sum(y[train_index]), sum(y[test_index]))\n",
    "        performance = forecast_performace(X_text, X_num, train_index, test_index, y, \"model/tenbyten_\"+\n",
    "                                          str(outer_loop)+\"_\"+str(inner_loop), model_type = \"text\", archi = \"CNN\")\n",
    "        with open(\"model/dl_text_cnn_10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "            for item in performance:\n",
    "                file.write(\"%s,\" % item)\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Example from : https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "import os\n",
    "word_index = pickle.load(open(\"data/10k/word_map.pickle\", \"rb\"))\n",
    "GLOVE_DIR = \"/shared/data/word_embedding/\"\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max(word_index.values()) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index_word = dict(zip(word_index.values(),word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maxlen = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_model_text(no_merge = True):\n",
    "    # this is the fast-text model\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    \n",
    "    embedding_layer = Embedding(max(word_index.values()) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(4))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    if no_merge:\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for year in range(2006, 2013):\n",
    "    print(year)\n",
    "    X_text, X_num, train_index, test_index, y = load_data(X_padded_text = X_padded, forecast_year = 1, random_split=False, test_train_split_year=year)\n",
    "    print(sum(y[train_index]), sum(y[test_index]))\n",
    "    forecast_performace(X_text, X_num, train_index, test_index,o y, \"text\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
