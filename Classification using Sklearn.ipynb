{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use traditional method in sklearn to forecasting bankcruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bankcruptcy prediction using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and align data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use SVD to reduce the dimension in the 10k text\n",
    "svd_dim = 50\n",
    "max_features = 10000\n",
    "def generate_save_SVD():\n",
    "    \"\"\"\n",
    "    Note: Deprecated. Should fit SVD on train set and tranform on test\n",
    "    \"\"\"\n",
    "    X = np.load('data/10k/X_tfidf.npy')\n",
    "    print(X.shape)\n",
    "    text = pd.DataFrame(data=X)\n",
    "    svd = TruncatedSVD(n_components=svd_dim)\n",
    "    svd_text = svd.fit_transform(text)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    np.save(\"data/10k/svd_X_tfidf.npy\", svd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NIAT', 'NISALE', 'OIADPAT', 'OIADPSALE', 'EBITAT', 'EBITDPAT', 'EBITSALE', 'SEQAT', 'REAT', 'LCTAT', 'LCTCHAT', 'LTAT', 'LOGSALE', 'CHAT', 'CHLCT', 'QALCT', 'ACTLCT', 'WCAPAT', 'LCTLT', 'INVTSALE', 'SALEAT', 'APSALE', 'LOGAT', 'INVCHINVT', 'CASHAT', 'LCTSALE', 'RELCT', 'FAT', 'SIGMA', 'NIMTA', 'LTMTA', 'CASHMTA', 'PRICE', 'RSIZE', 'EXCESS_RETURN', 'MBE']\n"
     ]
    }
   ],
   "source": [
    "# all numerical data\n",
    "final_variable = pd.read_csv('data/final_variables.csv')\n",
    "final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "drop_list = ['gvkey','datadate','fyear','cusip','PERMNO','PERMCO', 'Y']\n",
    "all_x_var = list(final_variable.drop(drop_list, axis=1))\n",
    "print(all_x_var)\n",
    "lasso_x = ['PRICE','OIADPAT','NIMTA','FAT','LCTCHAT','EXCESS_RETURN','LCTAT','EBITDPAT'] # lasso selected features\n",
    "non_svd_text = pd.DataFrame(data=np.load('data/10k/X_tfidf.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to load and align data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_year_before(df, n = 1):\n",
    "    \"\"\"input x,y df, return df with y and n year before x\"\"\"\n",
    "    dat_tmp = df.copy()\n",
    "    dat_tmp['fyear'] = dat_tmp['fyear'] + n\n",
    "    dat_tmp = dat_tmp.drop('Y',axis =1)\n",
    "    Ys = df[['fyear','gvkey','Y']]\n",
    "    n_year = pd.merge(dat_tmp,Ys,how = 'inner',on=['fyear','gvkey'])\n",
    "    return n_year\n",
    "\n",
    "\n",
    "def merge_data(svd=False, forecast_year = 1):\n",
    "    print('Loading data')\n",
    "    \n",
    "    # load accounting and market data\n",
    "    final_variable = pd.read_csv('data/final_variables.csv')\n",
    "    final_variable = final_variable.drop('Unnamed: 0',1)\n",
    "    final_variable = final_variable.replace([np.inf,-np.inf],0)\n",
    "    final_variable = final_variable.query('fyear <= 2014 & fyear >=1993')\n",
    "    final_variable.shape\n",
    "\n",
    "    index_10k = pd.read_csv('/shared/data/10k_2017/processed_corpus/10k_index.csv',usecols=['gvkey','fyear'])\n",
    "    print(\"concating\")\n",
    "    if svd:\n",
    "        text = pd.concat([svd_text, index_10k], axis=1)\n",
    "        text_idx = list(range(svd_dim))\n",
    "    if not svd:\n",
    "        text = pd.concat([non_svd_text, index_10k], axis=1)\n",
    "        text_idx = list(range(max_features))\n",
    "\n",
    "    print(\"merging\")\n",
    "    # combine text and numerical data\n",
    "    text_num = pd.merge(left=final_variable, right=text, how='inner', on=['gvkey','fyear'])\n",
    "    print(\"Total number of observations with no forecasting: \")\n",
    "    print(text_num.shape)\n",
    "    print(\"n year before\")\n",
    "    text_num_n_year = n_year_before(text_num, n = forecast_year)\n",
    "    print(\"Total number of observations: \")\n",
    "    print(text_num_n_year.shape)\n",
    "    text_num_n_year = text_num_n_year.sort_values(['gvkey', 'fyear'], ascending=[1, 1])\n",
    "    text_num_n_year = text_num_n_year.reset_index(drop=True)\n",
    "    return text_num_n_year, text_idx\n",
    "\n",
    "\n",
    "def load_data(how='text', svd=False, forecast_year = 1, \n",
    "              random_split = None, test_train_split_year = None):\n",
    "    \"\"\"Input user specified method, return train_x, train_y, test_x, test_y based on pre-load df\n",
    "    :param how: ['text','numerical','total']\n",
    "    :type how: str\n",
    "    :param svd: do decomposition to tfidf\n",
    "    :type svd: boolean\n",
    "    :random_split: If random split, need to load `data/split.pickle_1r` file that contains the train/test split\n",
    "    :test_train_split_year: Train data = less than this year; Test data = greater than or equal to this year\n",
    "    :return: train_x, train_y, test_x, test_y split by test_train_split_year or the split pickle file\n",
    "    :rtype: pandas.dataframe\n",
    "    \"\"\"\n",
    "    merged = merge_data(svd = svd, forecast_year = forecast_year)\n",
    "    text_num_n_year = merged[0]\n",
    "    text_idx = merged[1]\n",
    "    print(\"spliting\")\n",
    "    if random_split is not None:\n",
    "        all_index = text_num_n_year.index.tolist()\n",
    "#         split = sklearn.model_selection.train_test_split(all_index, train_size = 0.8, random_state=40)\n",
    "        split = pickle.load(open(\"data/split.pickle_\" + str(forecast_year) + \"r\" + str(random_split), \"rb\"))\n",
    "        train_index = split[0]\n",
    "        test_index = split[1]\n",
    "        train = text_num_n_year.iloc[train_index]\n",
    "        test = text_num_n_year.iloc[test_index]\n",
    "    else:\n",
    "        train = text_num_n_year[text_num_n_year['fyear'] < test_train_split_year]\n",
    "        test = text_num_n_year[text_num_n_year['fyear'] >= test_train_split_year]\n",
    "        \n",
    "    train_y = train['Y']\n",
    "    test_y = test['Y']\n",
    "\n",
    "    if how == 'text':\n",
    "        train_x = train.loc[:,text_idx]\n",
    "        test_x = test.loc[:,text_idx]\n",
    "        if svd:\n",
    "            print(\"Generating SVD\")\n",
    "            svd = TruncatedSVD(n_components=svd_dim)\n",
    "            train_x = svd.fit_transform(train_x)\n",
    "            test_x = svd.transform(test_x)\n",
    "            \n",
    "    if how == 'num':\n",
    "        # use 8 selected numerical features\n",
    "        train_x = train[all_x_var]\n",
    "        test_x = test[all_x_var]\n",
    "        \n",
    "    if how == 'total':\n",
    "        train_x_text = train.loc[:,text_idx]\n",
    "        test_x_text = test.loc[:,text_idx]\n",
    "        if svd:\n",
    "            print(\"Generating SVD\")\n",
    "            svd = TruncatedSVD(n_components=svd_dim)\n",
    "            train_x_text = svd.fit_transform(train_x_text)\n",
    "            test_x_text = svd.transform(test_x_text)\n",
    "        train_x = pd.concat([train_x_text,train[all_x_var]], axis=1) \n",
    "        test_x = pd.concat([test_x_text,test[all_x_var]], axis=1) \n",
    "    print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    # print(train_x.head(5))\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def get_prob_auc(model,x,y):\n",
    "    '''\n",
    "    input: a model and X y\n",
    "    ouput: AUC, Accuracy Ratio, and Brier Score\n",
    "    '''\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_yp = model.predict_proba(x)[:,1]\n",
    "    else:\n",
    "        pred_yp = model.decision_function(x)\n",
    "        pred_yp = (pred_yp - pred_yp.min()) / (pred_yp.max() - pred_yp.min())\n",
    "    \n",
    "    fpr,tpr,thresholds = roc_curve(y, pred_yp)\n",
    "    roc_auc = roc_auc_score(y, pred_yp)\n",
    "    accuracy_ratio = (roc_auc-0.5)*2\n",
    "    brier = metrics.brier_score_loss(y, pred_yp)\n",
    "    return roc_auc, accuracy_ratio, brier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dump_aligned_data_by_random_seed(random_state):\n",
    "    train_x, train_y, test_x, test_y = load_data(how=\"num\", svd=None, random_split = random_state)\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_num_1r\"+str(random_state), 'wb'), protocol=4)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = load_data(how=\"text\", svd=False, random_split = random_state)\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_text_1r\"+str(random_state), 'wb'), protocol=4)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = load_data(how=\"text\", svd=True, random_split = random_state)\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_textsvd_1r\"+str(random_state), 'wb'), protocol=4)\n",
    "\n",
    "#     train_x, train_y, test_x, test_y = load_data(how=\"total\", svd=False, random_split = random_state)\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_total_1r\"+str(random_state), 'wb'), protocol=4)\n",
    "\n",
    "#     train_x, train_y, test_x, test_y = load_data(how=\"total\", svd=True, random_split = random_state)\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_totalsvd_1r\"+str(random_state), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for random_state in range(1000, 1010):\n",
    "    dump_aligned_data_by_random_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_aligned_data_by_year(year):\n",
    "    \n",
    "#     train_x, train_y, test_x, test_y = load_data(how=\"num\", svd=True, random_split = False, test_train_split_year = year)\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_num_1_\" + str(year), 'wb'), protocol=4)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = load_data(how=\"text\", svd=False, random_split = False, test_train_split_year = year)\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_text_1_\"+ str(year), 'wb'), protocol=4)\n",
    "\n",
    "    train_x, train_y, test_x, test_y = load_data(how=\"text\", svd=True, random_split = False, test_train_split_year = year)\n",
    "    pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_textsvd_1_\"+ str(year), 'wb'), protocol=4)\n",
    "\n",
    "#     train_x, train_y, test_x, test_y = load_data(how=\"total\", svd=False, random_split = False, test_train_split_year = year)\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_total_1_\"+ str(year), 'wb'), protocol=4)\n",
    "\n",
    "#     train_x, train_y, test_x, test_y = load_data(how=\"total\", svd=True, random_split = False, test_train_split_year = year)\n",
    "#     pickle.dump([train_x, train_y, test_x, test_y], open(\"data/aligned/sklearn_totalsvd_1_\"+ str(year), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dump_aligned_data_by_year(2005)\n",
    "dump_aligned_data_by_year(2006)\n",
    "dump_aligned_data_by_year(2007)\n",
    "dump_aligned_data_by_year(2008)\n",
    "dump_aligned_data_by_year(2009)\n",
    "dump_aligned_data_by_year(2010)\n",
    "dump_aligned_data_by_year(2011)\n",
    "dump_aligned_data_by_year(2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_aligned_data(how='text', svd=False, forecast_year = 1, test_train_split_year = None, random_state = None):\n",
    "    [train_x, train_y, test_x, test_y] = [None, None, None, None]\n",
    "    suffix = \"r\" + str(random_state)\n",
    "    if test_train_split_year is not None:\n",
    "        suffix = \"_\" + str(test_train_split_year)\n",
    "    if how == \"text\":\n",
    "        if svd is True:\n",
    "            [train_x, train_y, test_x, test_y] = pickle.load(open(\"data/aligned/sklearn_textsvd_\" + str(forecast_year)+suffix, 'rb'))\n",
    "        else:\n",
    "            [train_x, train_y, test_x, test_y] = pickle.load(open(\"data/aligned/sklearn_text_\" + str(forecast_year)+suffix, 'rb'))\n",
    "    if how == \"num\":\n",
    "        [train_x, train_y, test_x, test_y] = pickle.load(open(\"data/aligned/sklearn_num_\"+ str(forecast_year) + suffix, 'rb'))\n",
    "    if how == \"total\":\n",
    "        if svd is True:\n",
    "            [train_x, train_y, test_x, test_y] = pickle.load(open(\"data/aligned/sklearn_totalsvd_\" + str(forecast_year) + suffix, 'rb'))\n",
    "        else:\n",
    "            [train_x, train_y, test_x, test_y] = pickle.load(open(\"data/aligned/sklearn_total_\" + str(forecast_year) + suffix, 'rb'))\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search model hyper-parameters using random search  \n",
    "see example: http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory method for final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0:1,1:100}\n",
    "\n",
    "def create_model(model_type):\n",
    "    # create your model using this function\n",
    "    if model_type == 'MNB':\n",
    "        model = MultinomialNB()\n",
    "    if model_type == 'lg-num':\n",
    "        model = LogisticRegression(C = 0.18,penalty='l1', class_weight=class_weight)\n",
    "    if model_type == 'lg-text':\n",
    "        model = LogisticRegression(C = 0.001,penalty='l2', class_weight=class_weight)\n",
    "    if model_type == 'lg-text-nosvd':\n",
    "        model = LogisticRegression(C = 0.1,penalty='l1', class_weight=class_weight)\n",
    "    if model_type == 'lg':\n",
    "        model = LogisticRegressionCV(Cs=10, class_weight=class_weight)\n",
    "    if model_type =='SVM-text':\n",
    "        model = LinearSVC(C=0.001, class_weight=class_weight)\n",
    "    if model_type =='SVC-text':\n",
    "        model = SVC(C=0.5, kernel='linear', class_weight=class_weight, probability = True)        \n",
    "    if model_type == 'SVM-num':\n",
    "        model = LinearSVC(C=0.002, class_weight=class_weight)\n",
    "    if model_type == 'SVM-total':\n",
    "        model = LinearSVC(C= 0.0005, class_weight=class_weight)\n",
    "    if model_type == 'random_forest':\n",
    "        model = RandomForestClassifier(max_depth = 8, n_estimators= 200, class_weight=class_weight)\n",
    "    if model_type == 'random_forest-num':\n",
    "        model = RandomForestClassifier(max_depth = 2, n_estimators= 50, class_weight=class_weight)\n",
    "    if model_type == \"boost-text\":\n",
    "        model = GradientBoostingClassifier(min_samples_leaf = 0.1, n_estimators=50, class_weight=class_weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only look at single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = ('text','lg', False)\n",
    "print('method is: ', method)\n",
    "train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "print(sum(test_y))\n",
    "model = create_model(model_type=method[1])\n",
    "model.fit(train_x, train_y)\n",
    "pred_yp = model.predict_proba(test_x)[:,1]\n",
    "roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "print('out sample AUC:')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = ('text','random_forest', False)\n",
    "print('method is: ', method)\n",
    "train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "print(sum(test_y))\n",
    "model = create_model(model_type=method[1])\n",
    "model.fit(train_x, train_y)\n",
    "pred_yp = model.predict_proba(test_x)[:,1]\n",
    "roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "print('out sample AUC:')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = ('text', 'MNB', False)\n",
    "print('method is: ', method)\n",
    "train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "print(sum(test_y))\n",
    "model = create_model(model_type=method[1])\n",
    "model.fit(train_x, train_y)\n",
    "pred_yp = model.predict_proba(test_x)[:,1]\n",
    "roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "print('out sample AUC:')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = ('text', 'SVM-text', False)\n",
    "print('method is: ', method)\n",
    "train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "print(sum(test_y))\n",
    "model = create_model(model_type=method[1])\n",
    "clf = CalibratedClassifierCV(model) \n",
    "clf.fit(train_x, train_y)\n",
    "pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "print('out sample AUC:')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for random_state in range(1000, 1010):\n",
    "    method = ('num','lg', None)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], random_state = random_state)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)\n",
    "    with open(\"model/num_by_random.txt\", 'a') as file:\n",
    "        file.write(\"lg\"+ \", \"+ str(random_state) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")\n",
    "        \n",
    "for random_state in range(1000, 1010):\n",
    "    method = ('text','lg', None)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], random_state = random_state)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)\n",
    "    with open(\"model/text_by_random.txt\", 'a') as file:\n",
    "        file.write(\"lg_nosvd\"+ \", \"+ str(random_state) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")\n",
    "        \n",
    "for random_state in range(1000, 1010):\n",
    "    method = ('text','lg', True)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], random_state = random_state)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)\n",
    "    with open(\"model/text_by_random.txt\", 'a') as file:\n",
    "        file.write(\"lg_svd\"+ \", \"+ str(random_state) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by year  -LG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in range(2006, 2013):\n",
    "    print(year)\n",
    "    method = ('num','lg', None)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year = year)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)\n",
    "    with open(\"model/num_by_year.txt\", 'a') as file:\n",
    "        file.write(\"lg\"+ \", \"+ str(year) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text - No SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in range(2005, 2013):\n",
    "    print(year)\n",
    "    method = ('text','lg', False)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year = year)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)\n",
    "    with open(\"model/text_by_year.txt\", 'a') as file:\n",
    "        file.write(\"lg_nosvd\"+ \", \"+ str(year) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text - SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(2006, 2013):\n",
    "    print(year)\n",
    "    method = ('text','lg', True)\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year = year)\n",
    "    print(train_x.shape, test_x.shape, sum(train_y), sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    with open(\"model/text_by_year.txt\", 'a') as file:\n",
    "        file.write(\"lg_svd\"+ \", \"+ str(year) + \", \" + str(1) + \", \" + str(roc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model(model_type=method[1])\n",
    "model.fit(train_x, train_y)\n",
    "with open('model/' + method[0] + \"_\" + method[1] + \".pickle\", 'wb') as handle:\n",
    "    pickle.dump(model, handle)\n",
    "if hasattr(model, 'predict_proba'):\n",
    "    pred_yp = model.predict_proba(test_x)[:,1]\n",
    "else:\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    clf = CalibratedClassifierCV(model) \n",
    "    clf.fit(train_x, train_y)\n",
    "    pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "print('out sample AUC:')\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_map = pickle.load(file=open(\"data/10k/word_map.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = dict(zip(sorted(word_map, key=word_map.get)[:10000], model.feature_importances_,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(features, key=features.get)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods = [('text','lg', False), ('text', 'SVM-text', False), ('text','random_forest', False)]\n",
    "\n",
    "for method in methods:\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "    print(sum(test_y))\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    with open('model/' + method[0] + \"_\" + method[1] + \".pickle\", 'wb') as handle:\n",
    "        pickle.dump(model, handle)\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    else:\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "        clf = CalibratedClassifierCV(model) \n",
    "        clf.fit(train_x, train_y)\n",
    "        pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "methods = [('num','lg', None),('num','random_forest-num', None),('num','SVM-num', None)]\n",
    "methods = [('num','random_forest-num', None)]\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year= None, random_state = 1004)\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    with open('model/' + method[0] + \"_\" + method[1] + \".pickle\", 'wb') as handle:\n",
    "        pickle.dump(model, handle)\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    else:\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "        clf = CalibratedClassifierCV(model) \n",
    "        clf.fit(train_x, train_y)\n",
    "        pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods = [('total','SVM-total', True)]\n",
    "\n",
    "for method in methods:\n",
    "    print('method is: ', method)\n",
    "    train_x, train_y, test_x, test_y = load_aligned_data(how=method[0], svd=method[2], test_train_split_year = 2008)\n",
    "    model = create_model(model_type=method[1])\n",
    "    model.fit(train_x, train_y)\n",
    "    with open('model/' + method[0] + \"_\" + method[1] + \".pickle\", 'wb') as handle:\n",
    "        pickle.dump(model, handle)\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        pred_yp = model.predict_proba(test_x)[:,1]\n",
    "    else:\n",
    "        from sklearn.calibration import CalibratedClassifierCV\n",
    "        clf = CalibratedClassifierCV(model) \n",
    "        clf.fit(train_x, train_y)\n",
    "        pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "    roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "    print('out sample AUC:')\n",
    "    print(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  10 x 10 cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_10_by_10(method):\n",
    "    for outer_loop, outer_split in enumerate(ten_by_ten_split):\n",
    "        for inner_loop, fold in enumerate(outer_split):\n",
    "            train_index = fold[0]\n",
    "            test_index = fold[1]\n",
    "            train = text_num_n_year.iloc[train_index]\n",
    "            test = text_num_n_year.iloc[test_index]\n",
    "            train_y = train['Y']\n",
    "            test_y = test['Y']\n",
    "            \n",
    "            how = method[0]\n",
    "            if how == 'text':\n",
    "                train_x = train.loc[:,text_idx]\n",
    "                test_x = test.loc[:,text_idx]\n",
    "\n",
    "            if how == 'num':\n",
    "                train_x = train[all_x_var]\n",
    "                test_x = test[all_x_var]\n",
    "                \n",
    "            model = create_model(model_type=method[1])\n",
    "            model.fit(train_x, train_y)\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred_yp = model.predict_proba(test_x)[:,1]\n",
    "            else:\n",
    "                from sklearn.calibration import CalibratedClassifierCV\n",
    "                clf = CalibratedClassifierCV(model) \n",
    "                clf.fit(train_x, train_y)\n",
    "                pred_yp = clf.predict_proba(test_x)[:,1]\n",
    "            roc = metrics.roc_auc_score(test_y, pred_yp)\n",
    "            with open(\"model/\"+ method[0] + \"_\" + \n",
    "                method[1] + \"_\" + \"10by10_y\"+str(forecast_year)+\".txt\", 'a') as file:\n",
    "                file.write(str(roc))\n",
    "                file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ten_by_ten_split = pickle.load(open(\"data/ten_by_ten_splits.pickle_1r1001\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "concating\n",
      "merging\n",
      "Total number of observations with no forecasting: \n",
      "(95987, 10043)\n",
      "n year before\n",
      "Total number of observations: \n",
      "(83746, 10043)\n"
     ]
    }
   ],
   "source": [
    "forecast_year = 1\n",
    "merged = merge_data(svd = False, forecast_year = forecast_year)\n",
    "text_num_n_year = merged[0]\n",
    "text_idx = merged[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "methods = [('text','lg-text-nosvd', False), ('text','random_forest', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods = [('num','lg-num', False), ('text','random_forest-num', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ten_by_ten_split = pickle.load(open(\"data/ten_by_ten_splits.pickle_2r1001\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_year = 2\n",
    "merged = merge_data(svd = False, forecast_year = forecast_year)\n",
    "text_num_n_year = merged[0]\n",
    "text_idx = merged[1]\n",
    "methods = [('text','lg-text-nosvd', False), ('text','random_forest', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)\n",
    "    \n",
    "methods = [('num','lg-num', False), ('text','random_forest-num', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ten_by_ten_split = pickle.load(open(\"data/ten_by_ten_splits.pickle_3r1001\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast_year = 3\n",
    "merged = merge_data(svd = False, forecast_year = forecast_year)\n",
    "text_num_n_year = merged[0]\n",
    "text_idx = merged[1]\n",
    "methods = [('text','lg-text-nosvd', False), ('text','random_forest', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)\n",
    "    \n",
    "methods = [('num','lg-num', False), ('text','random_forest-num', False)]\n",
    "for method in methods:\n",
    "    cv_10_by_10(method)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
